{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "894c5ac8-8d77-4071-ba5e-6fe771a81ffd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"transformers==4.37.2\" \"datasets==2.16.1\" \"peft==0.8.2\" \"accelerate==0.26.1\" \"bitsandbytes==0.46.1\" \"trl==0.7.10\" \"huggingface_hub[hf_xet]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de37e2e8-5a3d-4502-9497-2f6b60c28b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "823d5ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbe35a74-69f3-445f-ac15-6b8df5497187",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- --------------\n",
      "accelerate                0.26.1\n",
      "aiohappyeyeballs          2.6.1\n",
      "aiohttp                   3.12.14\n",
      "aiosignal                 1.4.0\n",
      "anyio                     4.9.0\n",
      "argon2-cffi               25.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 3.0.0\n",
      "async-lru                 2.0.5\n",
      "attrs                     25.3.0\n",
      "babel                     2.17.0\n",
      "beautifulsoup4            4.13.4\n",
      "bitsandbytes              0.43.1\n",
      "bleach                    6.2.0\n",
      "certifi                   2025.7.14\n",
      "cffi                      1.17.1\n",
      "charset-normalizer        3.4.2\n",
      "colorama                  0.4.6\n",
      "comm                      0.2.2\n",
      "datasets                  2.16.1\n",
      "debugpy                   1.8.15\n",
      "decorator                 5.2.1\n",
      "defusedxml                0.7.1\n",
      "dill                      0.3.7\n",
      "docstring_parser          0.17.0\n",
      "executing                 2.2.0\n",
      "fastjsonschema            2.21.1\n",
      "filelock                  3.18.0\n",
      "fqdn                      1.5.1\n",
      "frozenlist                1.7.0\n",
      "fsspec                    2023.10.0\n",
      "h11                       0.16.0\n",
      "hf-xet                    1.1.5\n",
      "httpcore                  1.0.9\n",
      "httpx                     0.28.1\n",
      "huggingface-hub           0.33.4\n",
      "idna                      3.10\n",
      "ipykernel                 6.30.0\n",
      "ipython                   9.4.0\n",
      "ipython_pygments_lexers   1.1.1\n",
      "ipywidgets                8.1.7\n",
      "isoduration               20.11.0\n",
      "jedi                      0.19.2\n",
      "Jinja2                    3.1.6\n",
      "json5                     0.12.0\n",
      "jsonpointer               3.0.0\n",
      "jsonschema                4.25.0\n",
      "jsonschema-specifications 2025.4.1\n",
      "jupyter                   1.1.1\n",
      "jupyter_client            8.6.3\n",
      "jupyter-console           6.6.3\n",
      "jupyter_core              5.8.1\n",
      "jupyter-events            0.12.0\n",
      "jupyter-lsp               2.2.6\n",
      "jupyter_server            2.16.0\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.4.5\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.27.3\n",
      "jupyterlab_widgets        3.0.15\n",
      "lark                      1.2.2\n",
      "markdown-it-py            3.0.0\n",
      "MarkupSafe                3.0.2\n",
      "matplotlib-inline         0.1.7\n",
      "mdurl                     0.1.2\n",
      "mistune                   3.1.3\n",
      "mpmath                    1.3.0\n",
      "multidict                 6.6.3\n",
      "multiprocess              0.70.15\n",
      "nbclient                  0.10.2\n",
      "nbconvert                 7.16.6\n",
      "nbformat                  5.10.4\n",
      "nest-asyncio              1.6.0\n",
      "networkx                  3.5\n",
      "notebook                  7.4.4\n",
      "notebook_shim             0.2.4\n",
      "numpy                     2.3.1\n",
      "overrides                 7.7.0\n",
      "packaging                 25.0\n",
      "pandas                    2.3.1\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.4\n",
      "peft                      0.8.2\n",
      "pillow                    11.0.0\n",
      "pip                       25.1.1\n",
      "platformdirs              4.3.8\n",
      "prometheus_client         0.22.1\n",
      "prompt_toolkit            3.0.51\n",
      "propcache                 0.3.2\n",
      "psutil                    7.0.0\n",
      "pure_eval                 0.2.3\n",
      "pyarrow                   21.0.0\n",
      "pyarrow-hotfix            0.7\n",
      "pycparser                 2.22\n",
      "Pygments                  2.19.2\n",
      "python-dateutil           2.9.0.post0\n",
      "python-json-logger        3.3.0\n",
      "pytz                      2025.2\n",
      "pywin32                   311\n",
      "pywinpty                  2.0.15\n",
      "PyYAML                    6.0.2\n",
      "pyzmq                     27.0.0\n",
      "referencing               0.36.2\n",
      "regex                     2024.11.6\n",
      "requests                  2.32.4\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rfc3987-syntax            1.1.0\n",
      "rich                      14.0.0\n",
      "rpds-py                   0.26.0\n",
      "safetensors               0.5.3\n",
      "Send2Trash                1.8.3\n",
      "setuptools                80.9.0\n",
      "shtab                     1.7.2\n",
      "six                       1.17.0\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.7\n",
      "stack-data                0.6.3\n",
      "sympy                     1.14.0\n",
      "terminado                 0.18.1\n",
      "tinycss2                  1.4.0\n",
      "tokenizers                0.15.2\n",
      "torch                     2.7.1+cu126\n",
      "torchaudio                2.7.1+cu126\n",
      "torchvision               0.22.1+cu126\n",
      "tornado                   6.5.1\n",
      "tqdm                      4.67.1\n",
      "traitlets                 5.14.3\n",
      "transformers              4.37.2\n",
      "trl                       0.7.10\n",
      "typeguard                 4.4.4\n",
      "types-python-dateutil     2.9.0.20250708\n",
      "typing_extensions         4.14.1\n",
      "tyro                      0.9.26\n",
      "tzdata                    2025.2\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.5.0\n",
      "wcwidth                   0.2.13\n",
      "webcolors                 24.11.1\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.8.0\n",
      "widgetsnbextension        4.0.14\n",
      "xxhash                    3.5.0\n",
      "yarl                      1.20.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf68d576-c48a-4fad-8787-289fe8315ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textwrap\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a25a393-6c98-4148-8113-c95c169dfe13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 1 GPU(s) detected.\n",
      "   - Primary GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Check for GPU availability ---\n",
    "if torch.cuda.is_available():\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"✅ {gpu_count} GPU(s) detected.\")\n",
    "    print(f\"   - Primary GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"⚠️ No GPU detected, falling back to CPU. This will be very slow.\")\n",
    "    device = \"cpu\"\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a5ce193-ea69-4ce7-8527-cc8c7337084c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define model, dataset, and directory paths\n",
    "# model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "model_id = \"microsoft/phi-2\"\n",
    "dataset_id = \"HuggingFaceH4/no_robots\"\n",
    "raw_model_dir = \"./phi-2/raw\"\n",
    "finetuned_model_dir = \"./phi-2/fine-tuned\"\n",
    "\n",
    "os.makedirs(raw_model_dir, exist_ok=True)\n",
    "os.makedirs(finetuned_model_dir, exist_ok=True)\n",
    "\n",
    "# 3. Configure Quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "becc2c3a-40bd-4d47-90d0-f4a33e79b5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d9e1c97e8442cca809cd33eac3f316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4356234b65854ed480fc71a7598d6ea9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67171ccb41324d28b9b5593770b85183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f09d286a3940e4bc191411dd9764d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f425631ae004a089240e701aff9ec2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. Load the tokenizer and the base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the model with the 4-bit quantization config.\n",
    "# `device_map=\"auto\"` will automatically place the model on your available GPU.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, quantization_config=bnb_config, device_map=\"auto\", trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "609bdf9d-9b95-4d90-9672-0943bf7e74fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully loaded on device: cuda:0\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model successfully loaded on device: {model.device}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c068ddc-a145-4d66-9d86-48ef4d8a0ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Base Model (Before Fine-Tuning) ---\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question for the model:  hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using a default chat template that implements the ChatML format (without BOS/EOS tokens!). If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Base Model Response ---\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Base Model Response ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Decode and print only the assistant's part of the response\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m<|assistant|>\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m.strip())\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m30\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "# 5. Interact with the Base Model (Before Fine-Tuning) ---\n",
    "print(\"\\n--- Testing Base Model (Before Fine-Tuning) ---\")\n",
    "\n",
    "# Get input from the user\n",
    "user_question = input(\"Enter your question for the model: \")\n",
    "\n",
    "# Define the chat structure for the prompt using the user's input\n",
    "chat = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful, respectful and honest assistant.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": user_question},\n",
    "]\n",
    "# Apply the chat template\n",
    "prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "# Tokenize the input and move to GPU\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False).to(\"cuda\")\n",
    "\n",
    "# Generate the response\n",
    "outputs = model.generate(**inputs, max_length=100)\n",
    "response = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "print(\"\\n--- Base Model Response ---\")\n",
    "# Decode and print only the assistant's part of the response\n",
    "print(response.split(\"<|assistant|>\")[1].strip())\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4d4aba6-a9ee-46d9-b0a9-610503f4fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Configure LoRA (Parameter-Efficient Fine-Tuning)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "# lora_config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=32,\n",
    "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "#     lora_dropout=0.05,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "# )\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"dense\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "700a44e4-b699-4285-87f8-0f4c64e09a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e06c60067e39494a855784406fcfc3aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c033af4257ed489bb9b48760e4527131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/950 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 7. Load the dataset\n",
    "dataset = load_dataset(dataset_id, split=\"train[:10%]\")\n",
    "\n",
    "# --- MODIFIED: Create a formatting function to handle batches ---\n",
    "# def formatting_func(batch):\n",
    "#     # The function receives a batch of examples and must return a list of formatted strings.\n",
    "#     # We iterate over each conversation in the batch.\n",
    "#     output_texts = []\n",
    "#     for i in range(len(batch['messages'])):\n",
    "#         # `batch['messages'][i]` is a single conversation (a list of dictionaries).\n",
    "#         # `apply_chat_template` formats this conversation into a single string.\n",
    "#         text = tokenizer.apply_chat_template(batch['messages'][i], tokenize=False)\n",
    "#         output_texts.append(text)\n",
    "#     return output_texts\n",
    "\n",
    "\n",
    "def formatting_func(batch):\n",
    "    output_texts = []\n",
    "    for i in range(len(batch[\"messages\"])):\n",
    "        messages = batch[\"messages\"][i]\n",
    "        # We're assuming a simple user-assistant turn for the instruction format\n",
    "        user_message = next(\n",
    "            (msg[\"content\"] for msg in messages if msg[\"role\"] == \"user\"), None\n",
    "        )\n",
    "        assistant_message = next(\n",
    "            (msg[\"content\"] for msg in messages if msg[\"role\"] == \"assistant\"), None\n",
    "        )\n",
    "\n",
    "        if user_message and assistant_message:\n",
    "            text = f\"Instruct: {user_message}\\nOutput: {assistant_message}\"\n",
    "            output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "\n",
    "# 8. Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    # output_dir=\"./tinyllama-no_robots-finetuned\",\n",
    "    output_dir=\"./phi2-no_robots-finetuned\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    logging_steps=10,\n",
    "    max_steps=100,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    ")\n",
    "\n",
    "# 9. Initialize the SFTTrainer\n",
    "# We use the new `formatting_func` that correctly processes batches.\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18ee50f4-ccff-4188-a479-6061de173b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'prompt_id', 'messages', 'category'],\n",
      "    num_rows: 950\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69abc010-8001-4c1b-bc15-0f9a934c7e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Fine-Tuning Process ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:18, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.820500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.883300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.781100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.830600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.882200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.820700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.747400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.734500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.820500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning completed!\n",
      "--- Saving final adapter ---\n",
      "Adapter saved!\n"
     ]
    }
   ],
   "source": [
    "# 10. Start the fine-tuning process\n",
    "print(\"\\n--- Starting Fine-Tuning Process ---\")\n",
    "trainer.train()\n",
    "print(\"Fine-tuning completed!\")\n",
    "\n",
    "print(\"--- Saving final adapter ---\")\n",
    "trainer.save_model()\n",
    "print(\"Adapter saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b182ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19f003a92b6d41eeaa5442d11e40fb1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "358b8c3ab35445c4989c27767444b841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2a89804c0f4d719693ee3f5e76d52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02babf18754e4d969d07bd1171e4b6c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359c49c78e754345a6b18b3f15eab87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The model we want to fine-tune\n",
    "model_id = \"microsoft/phi-2\"\n",
    "# The new model name for the fine-tuned version\n",
    "new_model_id = \"phi-2-no-robots-finetuned\"\n",
    "\n",
    "# Configuration for 4-bit quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load the model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map={\"\": 0},  # map model to the first GPU\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "# Phi-2 doesn't have a pad token, so we set it to the End-Of-Sequence token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training\n",
    "\n",
    "# Prepare the model for k-bit training\n",
    "model.config.use_cache = False\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687383a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\n",
    "    \"HuggingFaceH4/no_robots\", split=\"train[:1000]\"\n",
    ")  # Using 1000 samples for a quick fine-tune\n",
    "\n",
    "\n",
    "# Formatting function to structure the data\n",
    "def format_instruction(sample):\n",
    "    # The 'no_robots' dataset has a 'messages' field, which is a list of conversations.\n",
    "    # We'll take the first two messages as prompt and response.\n",
    "    prompt = sample[\"messages\"][0][\"content\"]\n",
    "    response = sample[\"messages\"][1][\"content\"]\n",
    "    return f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n{response}\"\n",
    "\n",
    "\n",
    "# You can see an example here\n",
    "# print(format_instruction(dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7601cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # Target modules for Phi-2\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"dense\",\n",
    "        \"fc1\",\n",
    "        \"fc2\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Get the PEFT model\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca9a085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=\"./results\",  # Output directory for checkpoints\n",
    "    num_train_epochs=1,  # A single epoch is often enough for fine-tuning\n",
    "    per_device_train_batch_size=2,  # Batch size per device\n",
    "    gradient_accumulation_steps=2,  # Accumulate gradients over 2 steps\n",
    "    optim=\"paged_adamw_32bit\",  # Memory-efficient optimizer\n",
    "    save_steps=50,  # Save a checkpoint every 50 steps\n",
    "    logging_steps=10,  # Log training metrics every 10 steps\n",
    "    learning_rate=2e-4,  # Learning rate\n",
    "    weight_decay=0.001,  # Weight decay for regularization\n",
    "    fp16=False,  # Use fp16 precision\n",
    "    bf16=True,  # Use bf16 precision for better performance on new GPUs\n",
    "    max_grad_norm=0.3,  # Gradient clipping\n",
    "    max_steps=-1,  # Number of training steps (overrides num_train_epochs)\n",
    "    warmup_ratio=0.03,  # Warmup ratio\n",
    "    group_by_length=True,  # Group sequences of similar length to save memory\n",
    "    lr_scheduler_type=\"constant\",  # Learning rate scheduler\n",
    "    report_to=\"tensorboard\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be447874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# The instruction you want to test\n",
    "prompt = \"Explain the importance of low-rank adaptation in large language models.\"\n",
    "\n",
    "# Set up the generation pipeline\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200\n",
    ")\n",
    "\n",
    "# Generate the response\n",
    "result = pipe(f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\")\n",
    "print(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cac2857c-0207-42ed-bea0-eb81f763efab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "# 11. Merge the model for inference\n",
    "base_model_for_merge = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True\n",
    ")\n",
    "# MODIFIED: The path should be the main output directory, not the specific checkpoint folder.\n",
    "# The trainer saves the final adapter config and weights to the root of the output directory.\n",
    "final_adapter_path = \"./tinyllama-no_robots-finetuned\"\n",
    "fine_tuned_model = PeftModel.from_pretrained(base_model_for_merge, final_adapter_path)\n",
    "fine_tuned_model = fine_tuned_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "05129976-5aa7-4ef9-b96e-a1efc8d30578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Fine-Tuned Model (After Fine-Tuning) ---\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question for the FINE-TUNED model:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fine-Tuned Model Response ---\n",
      "Hi! I'm a helpful, respectful and honest assistant. I'm glad you're here. Can you help me with a task? I need to write a thank you note to my boss. I'm not sure what to say. Can you help me? I'm really grateful for all the help I've received from you. Thank you! \n",
      "\n",
      "Best regards, \n",
      "[Your Name] \n",
      "[Your Address] \n",
      "[City, State ZIP Code] \n",
      "[Email Address] \n",
      "[Phone Number] \n",
      "[Date] \n",
      "\n",
      "[Your Name] \n",
      "[Your Address] \n",
      "[City, State ZIP Code] \n",
      "[Email Address] \n",
      "[Phone Number] \n",
      "[Date]\n"
     ]
    }
   ],
   "source": [
    "# 12. Test the fine-tuned model with the same prompt\n",
    "print(\"\\n--- Testing Fine-Tuned Model (After Fine-Tuning) ---\")\n",
    "# Get new input from the user for the fine-tuned model\n",
    "finetuned_user_question = input(\"Enter your question for the FINE-TUNED model: \")\n",
    "\n",
    "# Define the chat structure for the new prompt\n",
    "finetuned_chat = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful, respectful and honest assistant.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": finetuned_user_question},\n",
    "]\n",
    "# Apply the chat template to the new prompt\n",
    "finetuned_prompt = tokenizer.apply_chat_template(\n",
    "    finetuned_chat, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "\n",
    "# Tokenize the new input and move to the detected device\n",
    "inputs = tokenizer(\n",
    "    finetuned_prompt, return_tensors=\"pt\", return_attention_mask=False\n",
    ").to(device)\n",
    "\n",
    "# Generate the response\n",
    "outputs = fine_tuned_model.generate(**inputs, max_length=200)\n",
    "response = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "print(\"\\n--- Fine-Tuned Model Response ---\")\n",
    "print(response.split(\"<|assistant|>\")[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e4387e7-2ade-457a-a57e-b7b1e937f20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install textwrap\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2483eed2-294e-4975-bf50-0862f26e6139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=====================================================================================\n",
      "                                 --- COMPARISON ---                                  \n",
      "=====================================================================================\n",
      "\n",
      "QUESTION: hi\n",
      "\n",
      "-------------------------------------------------------------------------------------\n",
      "|               BASE MODEL               |             FINE-TUNED MODEL             |\n",
      "-------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'base_model_response_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Wrap text for clean side-by-side display\u001b[39;00m\n\u001b[32m     11\u001b[39m wrapper = textwrap.TextWrapper(width=\u001b[32m38\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m base_lines = wrapper.wrap(\u001b[43mbase_model_response_text\u001b[49m)\n\u001b[32m     13\u001b[39m finetuned_lines = wrapper.wrap(fine_tuned_model_response_text)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Make the number of lines in both response lists equal for easier printing\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'base_model_response_text' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 13 - Side-by-Side Comparison ---\n",
    "print(\"\\n\\n\" + \"=\" * 85)\n",
    "print(f\"{'--- COMPARISON ---':^85}\")\n",
    "print(\"=\" * 85)\n",
    "print(f\"\\nQUESTION: {user_question}\\n\")\n",
    "print(\"-\" * 85)\n",
    "print(f\"| {'BASE MODEL':^38} | {'FINE-TUNED MODEL':^40} |\")\n",
    "print(\"-\" * 85)\n",
    "\n",
    "# Wrap text for clean side-by-side display\n",
    "wrapper = textwrap.TextWrapper(width=38)\n",
    "base_lines = wrapper.wrap(base_model_response_text)\n",
    "finetuned_lines = wrapper.wrap(fine_tuned_model_response_text)\n",
    "\n",
    "# Make the number of lines in both response lists equal for easier printing\n",
    "max_lines = max(len(base_lines), len(finetuned_lines))\n",
    "base_lines += [\"\"] * (max_lines - len(base_lines))\n",
    "finetuned_lines += [\"\"] * (max_lines - len(finetuned_lines))\n",
    "\n",
    "# Print each line of the responses side-by-side\n",
    "for i in range(max_lines):\n",
    "    print(f\"| {base_lines[i]:<38} | {finetuned_lines[i]:<40} |\")\n",
    "\n",
    "print(\"-\" * 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4e381d87-db9a-4157-89dd-c677e3ce9401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loading models for interactive comparison ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "We need an `offload_dir` to dispatch this model according to this `device_map`, the following submodules need to be offloaded: base_model.model.model.layers.10, base_model.model.model.layers.11, base_model.model.model.layers.12, base_model.model.model.layers.13, base_model.model.model.layers.14, base_model.model.model.layers.15, base_model.model.model.layers.16, base_model.model.model.layers.17, base_model.model.model.layers.18, base_model.model.model.layers.19, base_model.model.model.layers.20, base_model.model.model.layers.21, base_model.model.model.norm, base_model.model.lm_head.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     13\u001b[39m base_model_for_merge = AutoModelForCausalLM.from_pretrained(\n\u001b[32m     14\u001b[39m     model_id,\n\u001b[32m     15\u001b[39m     torch_dtype=torch.bfloat16,\n\u001b[32m     16\u001b[39m     device_map=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m     trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     18\u001b[39m )\n\u001b[32m     19\u001b[39m final_adapter_path = \u001b[33m\"\u001b[39m\u001b[33m./tinyllama-no_robots-finetuned\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m fine_tuned_model = \u001b[43mPeftModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model_for_merge\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_adapter_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m fine_tuned_model = fine_tuned_model.merge_and_unload()\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Models ready for comparison.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\pygod\\slm\\slm-finetuning\\venv\\Lib\\site-packages\\peft\\peft_model.py:354\u001b[39m, in \u001b[36mPeftModel.from_pretrained\u001b[39m\u001b[34m(cls, model, model_id, adapter_name, is_trainable, config, **kwargs)\u001b[39m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    353\u001b[39m     model = MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config.task_type](model, config, adapter_name)\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\pygod\\slm\\slm-finetuning\\venv\\Lib\\site-packages\\peft\\peft_model.py:728\u001b[39m, in \u001b[36mPeftModel.load_adapter\u001b[39m\u001b[34m(self, model_id, adapter_name, is_trainable, **kwargs)\u001b[39m\n\u001b[32m    724\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    725\u001b[39m     device_map = infer_auto_device_map(\n\u001b[32m    726\u001b[39m         \u001b[38;5;28mself\u001b[39m, max_memory=max_memory, no_split_module_classes=no_split_module_classes\n\u001b[32m    727\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m \u001b[43mdispatch_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m    \u001b[49m\u001b[43moffload_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    732\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdispatch_model_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m hook = AlignDevicesHook(io_same_device=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.peft_config[adapter_name].is_prompt_learning:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\pygod\\slm\\slm-finetuning\\venv\\Lib\\site-packages\\accelerate\\big_modeling.py:371\u001b[39m, in \u001b[36mdispatch_model\u001b[39m\u001b[34m(model, device_map, main_device, state_dict, offload_dir, offload_index, offload_buffers, skip_keys, preload_module_classes, force_hooks)\u001b[39m\n\u001b[32m    369\u001b[39m disk_modules = [name \u001b[38;5;28;01mfor\u001b[39;00m name, device \u001b[38;5;129;01min\u001b[39;00m device_map.items() \u001b[38;5;28;01mif\u001b[39;00m device == \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    370\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m offload_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m offload_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(disk_modules) > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    372\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWe need an `offload_dir` to dispatch this model according to this `device_map`, the following submodules \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    373\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mneed to be offloaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(disk_modules)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    374\u001b[39m     )\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    376\u001b[39m     \u001b[38;5;28mlen\u001b[39m(disk_modules) > \u001b[32m0\u001b[39m\n\u001b[32m    377\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m offload_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    378\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m os.path.isdir(offload_dir) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isfile(os.path.join(offload_dir, \u001b[33m\"\u001b[39m\u001b[33mindex.json\u001b[39m\u001b[33m\"\u001b[39m)))\n\u001b[32m    379\u001b[39m ):\n\u001b[32m    380\u001b[39m     disk_state_dict = extract_submodules_state_dict(model.state_dict(), disk_modules)\n",
      "\u001b[31mValueError\u001b[39m: We need an `offload_dir` to dispatch this model according to this `device_map`, the following submodules need to be offloaded: base_model.model.model.layers.10, base_model.model.model.layers.11, base_model.model.model.layers.12, base_model.model.model.layers.13, base_model.model.model.layers.14, base_model.model.model.layers.15, base_model.model.model.layers.16, base_model.model.model.layers.17, base_model.model.model.layers.18, base_model.model.model.layers.19, base_model.model.model.layers.20, base_model.model.model.layers.21, base_model.model.model.norm, base_model.model.lm_head."
     ]
    }
   ],
   "source": [
    "# Load models for comparison\n",
    "print(\"\\n--- Loading models for interactive comparison ---\")\n",
    "\n",
    "# Load the original base model (quantized)\n",
    "base_model_for_comparison = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, quantization_config=bnb_config, device_map=\"auto\", trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load the fine-tuned model\n",
    "base_model_for_merge = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, torch_dtype=torch.bfloat16, device_map=\"auto\", trust_remote_code=True\n",
    ")\n",
    "final_adapter_path = \"./tinyllama-no_robots-finetuned\"\n",
    "fine_tuned_model = PeftModel.from_pretrained(base_model_for_merge, final_adapter_path)\n",
    "fine_tuned_model = fine_tuned_model.merge_and_unload()\n",
    "\n",
    "print(\"✅ Models ready for comparison.\")\n",
    "\n",
    "\n",
    "# --- Step 11 - Interactive Side-by-Side Comparison ---\n",
    "while True:\n",
    "    print(\"\\n\\n\" + \"=\" * 85)\n",
    "    print(f\"{'--- INTERACTIVE COMPARISON ---':^85}\")\n",
    "    print(\"=\" * 85)\n",
    "    user_question = input(\"Enter your question (or type 'quit' to exit): \")\n",
    "\n",
    "    if user_question.lower() in [\"quit\", \"exit\"]:\n",
    "        print(\"Exiting comparison.\")\n",
    "        break\n",
    "\n",
    "    # Define the chat structure for the prompt\n",
    "    chat = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful, respectful and honest assistant.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": user_question},\n",
    "    ]\n",
    "    # Apply the chat template\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        chat, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False).to(\n",
    "        \"cuda\"\n",
    "    )\n",
    "\n",
    "    # --- Generate from Base Model ---\n",
    "    base_outputs = base_model_for_comparison.generate(**inputs, max_length=200)\n",
    "    base_response = tokenizer.batch_decode(base_outputs)[0]\n",
    "    base_model_response_text = base_response.split(\"<|assistant|>\")[1].strip()\n",
    "\n",
    "    # --- Generate from Fine-Tuned Model ---\n",
    "    finetuned_outputs = fine_tuned_model.generate(**inputs, max_length=200)\n",
    "    finetuned_response = tokenizer.batch_decode(finetuned_outputs)[0]\n",
    "    fine_tuned_model_response_text = finetuned_response.split(\"<|assistant|>\")[\n",
    "        1\n",
    "    ].strip()\n",
    "\n",
    "    # --- Print Comparison Table ---\n",
    "    print(\"\\n\" + \"-\" * 85)\n",
    "    print(f\"| {'BASE MODEL':^38} | {'FINE-TUNED MODEL':^40} |\")\n",
    "    print(\"-\" * 85)\n",
    "\n",
    "    wrapper = textwrap.TextWrapper(width=38)\n",
    "    base_lines = wrapper.wrap(base_model_response_text)\n",
    "    finetuned_lines = wrapper.wrap(fine_tuned_model_response_text)\n",
    "\n",
    "    max_lines = max(len(base_lines), len(finetuned_lines))\n",
    "    base_lines += [\"\"] * (max_lines - len(base_lines))\n",
    "    finetuned_lines += [\"\"] * (max_lines - len(finetuned_lines))\n",
    "\n",
    "    for i in range(max_lines):\n",
    "        print(f\"| {base_lines[i]:<38} | {finetuned_lines[i]:<40} |\")\n",
    "\n",
    "    print(\"-\" * 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d195e2c8-3b77-4a5c-93fb-546664ffb8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
