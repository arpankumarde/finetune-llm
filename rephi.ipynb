{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4f08e0e-df74-4463-a287-73ab83246f58",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb346c1-cdc8-49f5-b5e6-77c044d2ec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"transformers==4.37.2\" \"datasets==2.16.1\" \"peft==0.8.2\" \"accelerate==0.26.1\" \"bitsandbytes==0.46.1\" \"trl==0.7.10\" \"huggingface_hub[hf_xet]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0df073e-ff43-41e2-a144-331678f42b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6c96f1-2934-42c7-9e59-c13a26ce02d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e03a41-81af-48b9-8bda-e6f897a98d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn matplotlib evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a874338-e21c-4bec-9bc7-e250dd0ba8ee",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea008646-fda5-4529-8ac2-460ad185ef20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 1 GPU(s) detected.\n",
      "   - Primary GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1. Import necessary libraries\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, \n",
    "    TrainingArguments, EarlyStoppingCallback\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from trl import SFTTrainer\n",
    "import warnings\n",
    "import textwrap\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers.trainer_utils import EvalPrediction\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Check for GPU availability ---\n",
    "if torch.cuda.is_available():\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"✅ {gpu_count} GPU(s) detected.\")\n",
    "    print(f\"   - Primary GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    print(\"⚠️ No GPU detected, falling back to CPU. This will be very slow.\")\n",
    "    device = \"cpu\"\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb261b5d-1e0e-4736-bb5a-70310b743246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0724 21:30:25.178000 10256 Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 1 GPU(s) detected.\n",
      "   - Primary GPU: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b72a9dd371486b81e16d9041eff405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model loaded on device: cuda:0\n",
      "------------------------------\n",
      "\n",
      "--- PHASE 1: TESTING BASE MODEL ---\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question for the BASE model:  technically explain Transformers in ML\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Base Model Response ---\n",
      "Transformers are a type of machine learning model that are commonly used for natural language processing tasks such as language translation, text summarization, and sentiment analysis. They are based on the concept of self-attention, which allows the model to focus on different parts of the input text at different times. Transformers consist of an encoder and a decoder, with the encoder responsible for extracting features from the input text and the decoder responsible for generating the output text. They have been shown to achieve state\n",
      "------------------------------\n",
      "--- Saving base model to ./phi-2/raw ---\n",
      "✅ Base model saved!\n",
      "------------------------------\n",
      "\n",
      "--- Starting Fine-Tuning Process for Phi-2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 08:42, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.223700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.021800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.292200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.264600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.068100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.244600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.129300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.061300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.229200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.250600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.022000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.268900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>2.076800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.109600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.014200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.152000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>2.140800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.043400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>2.183200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.096400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>2.209700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.215300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fine-tuning completed!\n",
      "✅ Fine-tuned adapter saved to ./phi-2/fine-tuned-adapter!\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 2. Define model, dataset, and new directory paths\n",
    "model_id = \"microsoft/phi-2\"\n",
    "dataset_id = \"HuggingFaceH4/no_robots\"\n",
    "raw_model_dir = \"./phi-2/raw\"\n",
    "finetuned_adapter_dir = \"./phi-2/fine-tuned-adapter-enhanced\"\n",
    "finetuned_merged_dir = \"./phi-2/fine-tuned-merged-enhanced\"\n",
    "offload_dir = \"./phi-2/offload\"\n",
    "logs_dir = \"./phi-2/logs\"\n",
    "results_dir = \"./phi-2/results\"\n",
    "\n",
    "os.makedirs(raw_model_dir, exist_ok=True)\n",
    "os.makedirs(finetuned_adapter_dir, exist_ok=True)\n",
    "os.makedirs(finetuned_merged_dir, exist_ok=True)\n",
    "os.makedirs(offload_dir, exist_ok=True)\n",
    "os.makedirs(logs_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# 3. Configure Quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# 4. Load Tokenizer and Base Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "print(f\"Base model loaded on device: {base_model.device}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- PHASE 1: Interact with the Base Model (Before Fine-Tuning) ---\n",
    "print(\"\\n--- PHASE 1: TESTING BASE MODEL ---\")\n",
    "user_question_base = input(\"Enter your question for the BASE model: \")\n",
    "prompt = f\"### Instruction:\\n{user_question_base}\\n\\n### Response:\\n\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device) \n",
    "base_outputs = base_model.generate(\n",
    "    **inputs, \n",
    "    max_new_tokens=150, \n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9\n",
    ")\n",
    "base_response = tokenizer.batch_decode(base_outputs)[0]\n",
    "print(\"\\n--- Base Model Response ---\")\n",
    "print(base_response.split(\"### Response:\\n\")[1].strip().replace(\"<|endoftext|>\", \"\"))\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- Save the raw model for later comparison ---\n",
    "print(f\"--- Saving base model to {raw_model_dir} ---\")\n",
    "base_model.save_pretrained(raw_model_dir)\n",
    "tokenizer.save_pretrained(raw_model_dir)\n",
    "print(\"✅ Base model saved!\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# 5. Configure LoRA and Fine-Tune\n",
    "peft_model = prepare_model_for_kbit_training(base_model)\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"dense\", \"fc1\", \"fc2\"],\n",
    "    lora_dropout=0.1, #0.05\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "peft_model = get_peft_model(peft_model, lora_config)\n",
    "peft_model.print_trainable_parameters()\n",
    "\n",
    "train_dataset = load_dataset(dataset_id, split=\"train[:20%]\")\n",
    "eval_dataset = load_dataset(dataset_id, split=\"train[20%:25%]\")\n",
    "\n",
    "# dataset = load_dataset(dataset_id, split=\"train[:10%]\")\n",
    "\n",
    "def formatting_func(batch):\n",
    "    output_texts = []\n",
    "    for i in range(len(batch['messages'])):\n",
    "        messages = batch['messages'][i]\n",
    "        user_message = next((msg['content'] for msg in messages if msg['role'] == 'user'), None)\n",
    "        assistant_message = next((msg['content'] for msg in messages if msg['role'] == 'assistant'), None)\n",
    "        if user_message and assistant_message:\n",
    "            text = f\"Instruct: {user_message}\\nOutput: {assistant_message}\"\n",
    "            output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=finetuned_adapter_dir, # Save checkpoints and final adapter here\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    logging_steps=5,\n",
    "    max_steps=110,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=lora_config,\n",
    "    formatting_func=formatting_func,\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "print(\"\\n--- Starting Fine-Tuning Process for Phi-2 ---\")\n",
    "trainer.train()\n",
    "print(\"✅ Fine-tuning completed!\")\n",
    "trainer.save_model() \n",
    "print(f\"✅ Fine-tuned adapter saved to {finetuned_adapter_dir}!\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aebe8d-b93b-4961-a4c9-5ef2e6f92794",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"\\n--- Starting Fine-Tuning Process for Phi-2 ---\")\n",
    "trainer.train()\n",
    "print(\"✅ Fine-tuning completed!\")\n",
    "trainer.save_model() # Saves the adapter to the output_dir\n",
    "print(f\"✅ Fine-tuned adapter saved to {finetuned_model_dir}!\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34cd85e8-879f-4961-bed3-04788785259d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Merging adapter and saving full fine-tuned model ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d66be9f9d64c5fb2804dbf99c3501e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fully merged fine-tuned model saved to ./phi-2/fine-tuned-merged!\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Merge adapter and save the full fine-tuned model to disk ---\n",
    "print(\"--- Merging adapter and saving full fine-tuned model ---\")\n",
    "# MODIFIED: Load the base model fully on the CPU for a stable merge.\n",
    "# This avoids device_map complexities during the merge operation.\n",
    "base_model_for_merge = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "fine_tuned_model = PeftModel.from_pretrained(\n",
    "    base_model_for_merge, \n",
    "    finetuned_adapter_dir,\n",
    ")\n",
    "fine_tuned_model = fine_tuned_model.merge_and_unload()\n",
    "fine_tuned_model.save_pretrained(finetuned_merged_dir)\n",
    "tokenizer.save_pretrained(finetuned_merged_dir)\n",
    "print(f\"✅ Fully merged fine-tuned model saved to {finetuned_merged_dir}!\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03383402-b946-4c00-90e6-bf6b20cf1f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Clearing all models from memory to free up VRAM ---\n",
      "✅ GPU Memory Cleared.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Clear all models from memory before comparison phases ---\n",
    "print(\"--- Clearing all models from memory to free up VRAM ---\")\n",
    "del base_model\n",
    "del peft_model\n",
    "del base_model_for_merge\n",
    "del fine_tuned_model\n",
    "torch.cuda.empty_cache()\n",
    "print(\"✅ GPU Memory Cleared.\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a7a58d4-8078-486b-a449-ba00f29e4975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- PHASE 2: TESTING FINE-TUNED MODEL ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f60b8862db48f5aa3ee920d783f52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk and cpu.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question for the FINE-TUNED model:  technically explain Transformers in ML\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fine-Tuned Model Response ---\n",
      "Transformers are a type of neural network architecture that are commonly used in natural language processing (NLP) tasks such as machine translation, text classification, and sentiment analysis. They are based on the idea of self-attention, which allows the model to focus on different parts of the input text at different times. Transformers consist of an encoder and a decoder, which are both made up of multiple layers of self-attention blocks. The encoder takes in the input text and outputs a sequence\n",
      "------------------------------\n",
      "\n",
      "\n",
      "=====================================================================================\n",
      "                       --- PHASE 3: INTERACTIVE COMPARISON ---                       \n",
      "=====================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question (or type 'quit' to exit):  What is your personal opinion on modern art?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RAW model for generation...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# --- Generate from RAW Model ---\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading RAW model for generation...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m raw_model_for_comparison = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_model_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m raw_outputs = raw_model_for_comparison.generate(**inputs, max_new_tokens=\u001b[32m100\u001b[39m, pad_token_id=tokenizer.eos_token_id)\n\u001b[32m     40\u001b[39m raw_response_text = tokenizer.batch_decode(raw_outputs)[\u001b[32m0\u001b[39m].split(\u001b[33m\"\u001b[39m\u001b[33mOutput:\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[32m1\u001b[39m].strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\pygod\\slm\\slm-finetuning\\venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:566\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m    565\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    570\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    571\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    572\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\pygod\\slm\\slm-finetuning\\venv\\Lib\\site-packages\\transformers\\modeling_utils.py:3790\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[39m\n\u001b[32m   3786\u001b[39m         device_map_without_lm_head = {\n\u001b[32m   3787\u001b[39m             key: device_map[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m device_map.keys() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m modules_to_not_convert\n\u001b[32m   3788\u001b[39m         }\n\u001b[32m   3789\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head.values() \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head.values():\n\u001b[32m-> \u001b[39m\u001b[32m3790\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3791\u001b[39m \u001b[38;5;250m                \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3792\u001b[39m \u001b[33;03m                Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\u001b[39;00m\n\u001b[32m   3793\u001b[39m \u001b[33;03m                the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\u001b[39;00m\n\u001b[32m   3794\u001b[39m \u001b[33;03m                these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\u001b[39;00m\n\u001b[32m   3795\u001b[39m \u001b[33;03m                `device_map` to `from_pretrained`. Check\u001b[39;00m\n\u001b[32m   3796\u001b[39m \u001b[33;03m                https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\u001b[39;00m\n\u001b[32m   3797\u001b[39m \u001b[33;03m                for more details.\u001b[39;00m\n\u001b[32m   3798\u001b[39m \u001b[33;03m                \"\"\"\u001b[39;00m\n\u001b[32m   3799\u001b[39m             )\n\u001b[32m   3800\u001b[39m         \u001b[38;5;28;01mdel\u001b[39;00m device_map_without_lm_head\n\u001b[32m   3802\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mValueError\u001b[39m: \n                        Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit\n                        the quantized model. If you want to dispatch the model on the CPU or the disk while keeping\n                        these modules in 32-bit, you need to set `load_in_8bit_fp32_cpu_offload=True` and pass a custom\n                        `device_map` to `from_pretrained`. Check\n                        https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu\n                        for more details.\n                        "
     ]
    }
   ],
   "source": [
    "# --- PHASE 2: Interact with the Fine-Tuned Model ---\n",
    "print(\"\\n--- PHASE 2: TESTING FINE-TUNED MODEL ---\")\n",
    "# Load the saved merged model from disk\n",
    "fine_tuned_model_for_test = AutoModelForCausalLM.from_pretrained(\n",
    "    finetuned_merged_dir,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    offload_folder=offload_dir\n",
    ")\n",
    "user_question_finetuned = input(\"Enter your question for the FINE-TUNED model: \")\n",
    "prompt = f\"Instruct: {user_question_finetuned}\\nOutput:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "finetuned_outputs = fine_tuned_model_for_test.generate(**inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
    "finetuned_response = tokenizer.batch_decode(finetuned_outputs)[0]\n",
    "print(\"\\n--- Fine-Tuned Model Response ---\")\n",
    "print(finetuned_response.split(\"Output:\")[1].strip())\n",
    "del fine_tuned_model_for_test # Clear memory\n",
    "torch.cuda.empty_cache()\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "442835d7-36ba-48e9-8710-1ab0386f0bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=====================================================================================\n",
      "                       --- PHASE 3: INTERACTIVE COMPARISON ---                       \n",
      "=====================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question (or type 'quit' to exit):  What is your personal opinion on modern art?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RAW model for generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93f9d05cfdb4338ae8736294e5b7198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW model unloaded.\n",
      "Loading FINE-TUNED model for generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81bea1bfc9ce4de98648e7efca572f63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINE-TUNED model unloaded.\n",
      "\n",
      "-------------------------------------------------------------------------------------\n",
      "|               RAW MODEL                |             FINE-TUNED MODEL             |\n",
      "-------------------------------------------------------------------------------------\n",
      "| I!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! | I think modern art is a reflection of    |\n",
      "| !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! | the times we live in. It's a way for     |\n",
      "| !!!!!!!!!!!!!!!!!!!!!!!!               | artists to express their thoughts and    |\n",
      "|                                        | emotions in a way that is unique to      |\n",
      "|                                        | them. I appreciate the creativity and    |\n",
      "|                                        | innovation that goes into modern art,    |\n",
      "|                                        | even if it's not my personal taste.      |\n",
      "-------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "=====================================================================================\n",
      "                       --- PHASE 3: INTERACTIVE COMPARISON ---                       \n",
      "=====================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question (or type 'quit' to exit):  Can you tell me a secret?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RAW model for generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9643d6082587493b83b726e2f8b1d9f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW model unloaded.\n",
      "Loading FINE-TUNED model for generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd14784695f542e4a80203e48533cf43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk and cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINE-TUNED model unloaded.\n",
      "\n",
      "-------------------------------------------------------------------------------------\n",
      "|               RAW MODEL                |             FINE-TUNED MODEL             |\n",
      "-------------------------------------------------------------------------------------\n",
      "| What!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! | Sure, what is it? <|endoftext|>          |\n",
      "| !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! |                                          |\n",
      "| !!!!!!!!!!!!!!!!!!!!!!!!!!!            |                                          |\n",
      "-------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "=====================================================================================\n",
      "                       --- PHASE 3: INTERACTIVE COMPARISON ---                       \n",
      "=====================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question (or type 'quit' to exit):  Give me some advice on how to procrastinate more effectively.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RAW model for generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0758e3a9fb6f40a9b21d4a030f772ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW model unloaded.\n",
      "Loading FINE-TUNED model for generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ddcaeedb8141f6ab8973f7a88f8601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk and cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINE-TUNED model unloaded.\n",
      "\n",
      "-------------------------------------------------------------------------------------\n",
      "|               RAW MODEL                |             FINE-TUNED MODEL             |\n",
      "-------------------------------------------------------------------------------------\n",
      "| Well!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! | Well, first of all, you should always    |\n",
      "| !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! | start with the most boring and tedious   |\n",
      "| !!!!!!!!!!!!!!!!!!!!!!!!!!!            | tasks, because that way you can avoid    |\n",
      "|                                        | the ones that actually matter. Then,     |\n",
      "|                                        | you should take frequent breaks and      |\n",
      "|                                        | reward yourself with snacks and games.   |\n",
      "|                                        | And finally, you should never finish     |\n",
      "|                                        | anything, because that would be too      |\n",
      "|                                        | easy and boring. Trust me, this is the   |\n",
      "|                                        | best way to procrastinate.               |\n",
      "-------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "=====================================================================================\n",
      "                       --- PHASE 3: INTERACTIVE COMPARISON ---                       \n",
      "=====================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your question (or type 'quit' to exit):  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting comparison.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    print(\"\\n\\n\" + \"=\"*85)\n",
    "    print(f\"{'--- PHASE 3: INTERACTIVE COMPARISON ---':^85}\")\n",
    "    print(\"=\"*85)\n",
    "    user_question_comp = input(\"Enter your question (or type 'quit' to exit): \")\n",
    "    \n",
    "    if user_question_comp.lower() in ['quit', 'exit']:\n",
    "        print(\"Exiting comparison.\")\n",
    "        break\n",
    "        \n",
    "    prompt = f\"Instruct: {user_question_comp}\\nOutput:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # --- Generate from RAW Model ---\n",
    "    print(\"Loading RAW model for generation...\")\n",
    "    # MODIFIED: Explicitly set device_map to the target device instead of \"auto\"\n",
    "    raw_model_for_comparison = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, \n",
    "        quantization_config=bnb_config, \n",
    "        device_map=device, \n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    raw_outputs = raw_model_for_comparison.generate(**inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
    "    raw_response_text = tokenizer.batch_decode(raw_outputs)[0].split(\"Output:\")[1].strip()\n",
    "    del raw_model_for_comparison\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"RAW model unloaded.\")\n",
    "\n",
    "    # --- Generate from FINE-TUNED Model ---\n",
    "    print(\"Loading FINE-TUNED model for generation...\")\n",
    "    fine_tuned_model_for_comp = AutoModelForCausalLM.from_pretrained(\n",
    "        finetuned_merged_dir, \n",
    "        device_map=\"auto\", \n",
    "        trust_remote_code=True, \n",
    "        offload_folder=offload_dir\n",
    "    )\n",
    "    finetuned_outputs_comp = fine_tuned_model_for_comp.generate(**inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
    "    finetuned_response_text = tokenizer.batch_decode(finetuned_outputs_comp)[0].split(\"Output:\")[1].strip()\n",
    "    del fine_tuned_model_for_comp\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"FINE-TUNED model unloaded.\")\n",
    "\n",
    "    # --- Print Comparison Table ---\n",
    "    print(\"\\n\" + \"-\" * 85)\n",
    "    print(f\"| {'RAW MODEL':^38} | {'FINE-TUNED MODEL':^40} |\")\n",
    "    print(\"-\" * 85)\n",
    "\n",
    "    wrapper = textwrap.TextWrapper(width=38)\n",
    "    raw_lines = wrapper.wrap(raw_response_text)\n",
    "    finetuned_lines = wrapper.wrap(finetuned_response_text)\n",
    "\n",
    "    max_lines = max(len(raw_lines), len(finetuned_lines))\n",
    "    raw_lines += [''] * (max_lines - len(raw_lines))\n",
    "    finetuned_lines += [''] * (max_lines - len(finetuned_lines))\n",
    "\n",
    "    for i in range(max_lines):\n",
    "        print(f\"| {raw_lines[i]:<38} | {finetuned_lines[i]:<40} |\")\n",
    "\n",
    "    print(\"-\" * 85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1113cc16-6568-4ceb-80cd-0df4293955d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
